package ac.keio.sslab.nlp;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileReader;

import org.apache.commons.cli.Options;
import org.apache.hadoop.fs.Path;

import ac.keio.sslab.clustering.bottomup.BottomupClassifier;
import ac.keio.sslab.clustering.view.PointDumper;
import ac.keio.sslab.utils.SimpleGitReader;

public class PatchClusterJob extends SingletonGroupNLPJob {

	@Override
	public String getJobName() {
		return "bottomup.simpatch";
	}

	@Override
	public String getShortJobName() {
		return "pc";
	}

	@Override
	public NLPJobGroup getParentJobGroup() {
		return new BottomUpJob();
	}

	@Override
	public File getLocalJobDir() {
		return null;
	}

	@Override
	public Path getHDFSJobDir() {
		return null;
	}

	@Override
	public String getJobDescription() {
		return "show similar patches for a patch";
	}

	@Override
	public Options getOptions() {
		Options opt = new Options();
		opt.addOption("p", "patchID", true, "patch ID in idIndex generated by corpus job");
		opt.addOption("s", "sha", true, "commit sha");
		return opt;
	}

	@Override
	public void run(JobManager mgr) throws Exception {
		NLPConf conf = NLPConf.getInstance();
		JobManager bottomupMgr = mgr.getParentJobManager();
		JobManager corpusMgr = bottomupMgr.getParentJobManager().getParentJobManager();
		corpusMgr.lock();
		File bottomupDir = bottomupMgr.getLocalOutputDir();
		File corpusDir = corpusMgr.getLocalOutputDir();
		if (!corpusMgr.hasArg("g")) {
			throw new Exception("Not implemented for corpora from corpus.text job");
		}
		File gitDir = new File(corpusMgr.getArgStr("g"));
		File classDir = new File(conf.finalOutputFile, "patchCluster/" + mgr.getArgStr("j"));
		int patchID = mgr.getArgOrDefault("p", -1, Integer.class);
		String sha = mgr.getArgOrDefault("s", null, String.class);
		if ((patchID == -1 && sha == null) || (patchID != -1 && sha != null)) {
			System.err.println("Specify either -p or -s");
			return;
		}

		if (sha != null) {
			File idIndex = new File(corpusDir, "/idIndex.txt");
			BufferedReader br = new BufferedReader(new FileReader(idIndex));
			String line = null;
			boolean found = false;
			while ((line = br.readLine()) != null) {
				if (!line.contains(sha)) {
					continue;
				}
				patchID = Integer.parseInt(line.split("\t")[0]);
				found = true;
				break;
			}
			br.close();
			if (!found) {
				System.err.println("Could not find sha " + sha + " in " + idIndex.getAbsolutePath());
				return;
			}
			System.out.println("Use patchID = " + patchID + " for " + sha);
		}

		SimpleGitReader git = new SimpleGitReader(gitDir);
		PointDumper p = PointDumper.readJson(classDir, patchID);
		if (p == null) {
			System.out.println("Seems like we could not find the file for " + patchID + " in " + classDir.getAbsolutePath());
			System.out.println("Extract clusters for the patch. wait a moment...");
			File clustersFile = new File(bottomupDir, "/clusters.csv");
			File corpusIDIndexFile = new File(bottomupDir, "/corpusIDIndex.csv");
			File idIndexFile = new File(corpusDir, "/idIndex.txt");
			BottomupClassifier c = new BottomupClassifier(clustersFile, corpusIDIndexFile, idIndexFile, gitDir);
			if (!c.getPointIDs().contains(patchID)) {
				System.err.println("Invalid patch ID: " + patchID);
				return;
			}
			c.writeBestClusterJson(classDir, patchID);
			System.out.println("Finished! Retry read json for " + patchID);
			p = PointDumper.readJson(classDir, patchID);
		}
		System.out.println(p.toPlainText());
		System.out.println(p.getClusterMetrics().toPlainText(git));
		corpusMgr.unLock();
	}

	@Override
	public boolean runInBackground() {
		return false;
	}

}
